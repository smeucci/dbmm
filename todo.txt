ELABORATO DBMM:


- FATTO - automatizzare download delle immagini: fare in modo che l'url cicli in modo da prendere le prime 28 immagini di query e poi le seconde 28, 
e così via fine ad un numero desiderato.
- FATTO - fixare il fatto che non ti crea la cartella per la query 
- IN SOSPESO - prendere in maniera automatica una lista di identità (football-data) e automatizzare il download per tutte queste identità
- trovare lista di politici
- usare lista di attori da vgg-face.mat
- IN LAVORAZIONE - ottimizzare in termini di dimensioni immagini (per successivo crop per cnn) per storage
- IN LAVORAZIONE - implementare in matlab il detector delle faccia ed il crop automatico con vgg
- implementare downloader anche per google search (bing già fatto): problematico, metodo analogo a bing non funziona; usare custom search engine non pratico a meno di non pagare
(gratis 100 richieste al giorno, dieci risultati per query)
- provare image reverse search per google
- provare yahoo e baidu
- IN SOSPESO - implementare threading anche per le altre funzioni
- implementare parallelismo su matlab per detector
- FATTO - salvare il rank dell'immagine in modo da denominarle correttamente in base ad esso
- FATTO - salvo solo identità e loro stato. (salvare href su db)
- FATTO - riogarnizzare threading
- FATTO - cambiare numero di href se rilasso la condizione sulla dimensione dell'immagine; posso anche toglierla del tutto ma a quel punto occorrerà fare un resize per le immagini troppo grandi.
Per ora con query 'Leo Messi' trovo circa 500 immagini prima che si ripetano sistematicamente (ci saranno doppioni anche prima sicuramente).
- FATTO - implementare la classe per le immagini

PRIORITA':
- FATTO - salvare su db url di immagini
- FATTO - script che prende solo gli url e li salva su db, il resto dopo
- FATTO - implementare proxy
- FATTO - fare fetcher e parser per altri se
- FATTO - implementare downloader: prende url da db, scarica e li salva su disco con multithreading
- FATTO - fare in modo che un crash non porti a perdita di dati già acquisiti (immagini, file dei path etc) e che si possa ripristinare il download da l'ultima query effettuata
- SOLO Downloader - implementare parpool su matlab per collector e downloader
- FATTO - sistemare salvataggio nella cartella crop
- FATTO - sistemare come prendo la differenza di liste
- FATTO - sistemare url yahoo
- FATTO - implementare controllo su numero di href (es. < 7 problema)
- FATTO - creare lista di identità come subset dei 2622
- FATTO - gestire eccezione in face detection matlab, invalid input; implementato con dlib
- FATTO - usare dlib per face detection in locale, imparare ad usarlo e verificare performance
- FATTO - fare in modo che i file vengano salvati secondo il loro corretto formato.
- trovare il modo migliore per croppare l'immagine date le coord di box
- mettere gcp
- stampare fine detect e crop identity+se
- studiare net di vgg, guardare laboratorio 3, matconvnet

- FATTO - face detection, salva solo le coordinate del bounding box, originale ed allargato, senza salvare nuove immagini, tieni solo l'originale
- FATTO - duplicate removal, usando vlad, uso bounding box allargata per togliere i duplicati da un'identità; cluster + selezione migliore.
- provare dsift modficando numero di patch locali per immagine.
- FATTO - verificare se la pre-trained cnn riconosce le sue identità (si)
- usare la rete come in lab3 face_recognition, usare bounding box originale (stretta), estrarre descrittore fc layer
- addestrare linear svm con le prime 50 img per se e mostrare a gruppi di 50 i risultati della classificazione per identity (verde, rosso) (rimozione impurità + validazione visuale)

TRAIN
- prendo prime 50 img per ogni classe ed estraggo fc layer dalla pre trained cnn.
- fare in modo di prenderle in ordine giusto
- addestro svm e salvo il modello.
- per ogni immagine usata per training aggiungo field nella struct con 1, indicando che è true che appartiene a quella classe

TEST
- prendo tutte le immagini di una classe, una classe per volta.
- estraggo i descrittori
- faccio predizione usando il modello salvato.
- in base alla predizione assegno il valore al nuovo campo della struct, 1 se la predizione mi dice che appartiene alla classe che mi aspetto, 0 altrimenti.

- alla fine ho la solita variabile dataset che contiene per tutte le classe, tutte le immagini con un campo che mi dice se quella immagine corrisponde veramente, secondo il
classificatore, alla classe in cui si trova.
- in base a dataset, creo file di testo (meglio usare db) per ogni classe e con queste informazioni, che userò poi in php per la validazione visuale web.

######################################
- provare 800x3 con le prime quattro identità
- fare face detection di queste e poi duplicate removal.
- se tutto va bene, proseguire con le altre identità, su una nuova cartella, cancellando le vecchie dopo che è tutto ok per quella identità.


DA FARE:
- nel database dataset, estendere tabella identities con colonne: num_detected, num_unique
- creare tabella detection, corrisponde a informazioni in dataset-1000.mat
- creare tabella unique, corrisponde a informazione dataset-unique1000.mat
- sistemare codice in modo che lavori con il database invece che con i file .mat
- cambiare nome database da collector_new a collector ed eliminare il vecchio collector
- fare backup e cancellare roba che non serve più
- pulizia generale anche dei file sorgente


RELAZIONE:
- necessità di database grandi per cnn e difficoltà a trovare dataset gratuiti e/o difficoltà a creare dataset nuovi soprattutto l'operazione di annotazione
- quindi creazione di pipeline per creazione di dataset già annotati
- due fasi principali: creazione dataset e pulizia dataset
- trovare lista di identità, citare articolo deep face rec
- fetch di url da vari se con python, uso database
- download di immagini con python
- face detector con dlib, descrizione di come funziona dlib e come l'ho implementato (c++, cmake, etc)
- necessità di rimozione errori nel dataset tramite due fasi
- duplicate removal, calcolo sift/dsift, generazione codebook, vlad (descrizione), clustering e rimozione duplicati, cit articolo su vlad
- fc layer della cnn già addestrata, come perchè, cit slide prof?
- addestramento linear svm con fc layer prendendo primi 50 immagini per se per ogni identity, e verificare l'identity delle altre immagini
- infine validazione visuale tramite web interface
- ottengo database grande annotato e pulito per cnn.

- cambiare nome sul server a collector_new etc

ESECUZIONE:
- fetch and downloaded primi 100 con check su num
- lanciare script per crop di questi 100
- i prossimi 100 provare fetch senza check su numZ (no), semmai mettere pausa tra un'identità e l'altra

FUNZIONAMENTO:
- data una lista di identità, devo recuperare le immagini relative e cropparle per cnn:
    - lista di identità: la prendo da vgg o la trovo da qualche parte, la istanzia su matlab.
    - recupero immagini: tramite script python chiamato da matlab, scarica, data una query, le n immagini relative, ne salva il path ed associa una label.
    - crop: usando un face detector su matlab, una volta restituita un insieme di immagini per una query, trova la faccia e croppa l'immagine.
    
QUESTIONI:

- decidere se è meglio scaricare tutte le immagini per tutte le identità e poi cropparle oppure scaricare un insieme di immagini per un'identità, cropparle e poi passare
all'identità successiva. (risparmio in termini di storage necessario).
- decidere il formato delle immagini da scaricare. Una risoluzione troppo elevata diventa onerosa da sostenere per il face detector (almeno in locale). 
Limitare immagini a 1MB? Oppure prendere immagini nella categoria small? Tanto il crop fa uno scaling.
- capire come integrare le varie ricerche su bing e google. Due chiamate diverse in matlab? Oppure un unica chiama allo stesso script che poi effettua entrambe le ricerche?
- posso prima fare uno scraping generale degli href dai vari engine per tutte le entità e salvare gli url nel database. Successivamente scaricare, detect etc
-  la lista delle identità la prendo da matlab che chiama poi collector, o da collector leggo lista e chiamo search?: con matlab posso usare parpool, 4 utenti insieme, da python multiprocessing

https://cse.google.com/cse?searchtype=image&start=1&num=20&q=monkey&client=google-csbe&output=json&cx=005294867147720465028:jm1ktvooogg#gsc.tab=1&gsc.q=monkey&gsc.page=17

http://c2.staticflickr.com//6//5498//14065076288_1558e6dcd1_b.jpg

http://image.baidu.com/search/index?tn=baiduimage&word=george%20clooney&pn=1

http://www.picsearch.com/index.cgi?start=0&q=leo%20messi

turlL":[\'"]?([^\'" <>]+)  iurl

https://duckduckgo.com/i.js?q=leo+messi&s=1&s=4

http://fresh-proxy.appspot.com/www.bing.com/images/search?q=leo+messi&first=1&count=28

https://it.images.search.yahoo.com/search/images?p=leo+messi&b=3
