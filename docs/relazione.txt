ELABORATO DBMM


RELAZIONE:
- necessità di database grandi per cnn e difficoltà a trovare dataset gratuiti e/o difficoltà a creare dataset nuovi soprattutto l'operazione di annotazione
- quindi creazione di pipeline per creazione di dataset già annotati
- due fasi principali: creazione dataset e pulizia dataset
- trovare lista di identità, citare articolo deep face rec
- fetch di url da vari se con python, uso database
- download di immagini con python
- face detector con dlib, descrizione di come funziona dlib e come l'ho implementato (c++, cmake, etc)
- necessità di rimozione errori nel dataset tramite due fasi
- duplicate removal, calcolo sift/dsift, generazione codebook, vlad (descrizione), clustering e rimozione duplicati, cit articolo su vlad
- fc layer della cnn già addestrata, come perchè, cit slide prof?
- addestramento linear svm con fc layer prendendo primi 50 immagini per se per ogni identity, e verificare l'identity delle altre immagini
- infine validazione visuale tramite web interface
- ottengo database grande annotato e pulito per cnn.

- IMMAGINI:
    - esempio immagini di download, pulita e con più facce e che non c'entra niente
    - confronto immagine originale con quella dopo face detector, mostrare anche immagine originale con disegnata bounding box allargata
    - esempio di due duplicati che non si rimuovono perchè immagini originali troppo diverse anche se mostrano lo stesso contenuto
    - mostrare immagine originale con disegnata bounding box originale usata per la classificazione
    - mostrare il tool web per la validazione, sia per la classificazione che per la validazione
    - grafici, tabelle per la parte di validazione

- REFERENCES:
    - articolo fonte
    - libsvm
    - matconvet
    - vlfeat
    - dlib

- INTRODUZIONE
Le moderne reti neurali convoluzionali (CNNs) mostrano sempre più successi promettenti. Tuttavia per addestrarle e farle rendere al meglio, esse richiedono di una quantità di dati enormi per essere addestrate. Acquisire tali quantità di dati non è semplice: molti dataset di tali dimensioni sono a pagamento; inoltre creare da zero una dataset da milioni di contenuti (es. immagini) richiede uno sforzo notevole, non solo in termini di acquisizione del materiale, ma anche dal punto di vista dell'annotazione, ovvero dalla creazione di un groundtruth, fondamentale per qualsiasi tecnica di training dei dati. Questo aspetto spesso rappresenta un collo di bottiglia nella ricerca in questo ambito, dove solo laboratori di ricerca con importanti risorse possono permettersi di creare dataset di tali dimensioni e quindi ottenere risultati ed esperimenti che siano significativi in tale ambito, non essendo elevato il numero di dataset di grandi dimensioni resi pubblici. L'obiettivo di questo elaborato è la creazione di una procedura grazie alla quale è possibile ottenere un dataset di grandi dimensioni, sfruttando le risorse dei motori di ricerca, e allo stesso tempo ridurre al minimo il tempo umano necessario all'acquisizione delle immagini e all'annotazione delle stesse tale varie fasi di collezioni delle risorse e di pulizia e classificazione delle stesse, lasciando all'utente umano solo la fase finale di validazione dei risultati automaticamente ottenuti. Quindi tale pipeline, procedura, si divide in due fasi fondamentali: creazione del dataset, ovvero acquisizione delle immagini, e validazione del dataset, ovvero validazione visuale dei risultati ottenuti automaticamente al fine di pulire e rendere utilizzabile il dataset ottenuto. La procedura creata si occupa in particolare della creazione di dataset di immagini contenenti facce.

- CREAZIONE DATASET
La prima fase di creazione del dataset si base sull'acquisizione di tale immagini sfruttando i moderni motori di ricerca. E' necessario quindi ottenere una lista di personalità che siano sufficientemente celebri, in modo che per ciascuna identità ci siano un numero elevato ( tra 500 e 1000) di immagini disponibili sui motori di ricerca.
Una volta fissata la lista di identità di cui si vogliono ottenere le immagini, si può passare all'acquisizione delle stesse.
La procedura sviluppata divide tale acquisizione in due parti: una fase di collezione e una fase di download.
Fase di collezione: la fase di collezione sfrutta tre motori di ricerca, ovvero bing, yahoo, aol per fare query ed ottenere le immagini per un'identità specificata. Tale query restituisce la pagina html che si vede in un browser, ovvero una lista di immagini con la possibilità di scrollare in basso la pagina per ottenerne di nuove. Fare una richiesta html a tale pagina restituisce solo una numero limitato di immagini per identità; è quindi necessario ripetere un certo numero di volte la query, incrementando opportunamente il numero di pagina (un offset)  per non ottenere le stesse immagini ogni volta. In queste pagine html saranno presenti dei link alle immagini relative alla ricerca. Ogni motore di ricerca ha un diverso layout della pagina html di risposta ed un diverso modo di presentare tali link; è quindi necessario creare dei parser ad hoc per ciascuno motore di ricerca utilizzato in modo da estrapolare correttamente i link. Inoltre essendo tali motori di ricerca suscettibili a molteplici richiede successive, sono stati utilizzati due proxy che si alternavano i motori di ricerca in modo da bilanciare il numero di richieste in modo da non saturarle e rendere vana la query. Una volta estrapolati i link dalle pagine html relative ad un'identità, questi veniva saltati su un database insieme alla motore di ricerca utilizzato per ottenerle, il ranking, e l'identità associata.
Fase di download: una volta costrutito il database di link, si può procedere alla fase di download e creazione fisica di un dataset di immagini associato alle varie identità. Questa fase non richiede particolare accorgimenti come l'uso di proxy per mascherare il propri ip, poichè il numero di link che provengono dalla stesso indirizzo ip, e quindi il numero di richieste fatte a questo e irrisorio rispetto alla fase precedente dove l'indirizzo era il solito, ovvero quello dei motori di ricerca.
Dopo queste due fasi ottengo la creazione di un dataset di immagini prese automaticamente dal web associate a ciascuna identità. Le dimensioni di tale dataset sono personalizzabili in base al numero di identità di cui si vuole effettuare la query e al numero di link da ottenere per ciascuna identità dai vari motori di ricerca.

- FACE DETECTOR
La fase precendete aveva il compito di acquisire fisicamente (digitalmente) le immagini per ottenere un dataset di grandi dimensioni. Le fasi successive hanno invece il compito di pulire tale dataset da immagini non pertinenti con l'identità a cui si riferiscono. Infatti facendo una ricerca utilizzando come query, ad esempio, una celebrità, restituire una grande quantità di immagini non tutte le quali conterrano l'identità stessa, ma ci saranno errori, immagini che non c'entrano nulla, eccetera. Quindi occore ripulire il dataset ottenuto per ciascuna identità. Inoltre l'obiettivo di questo elaborato è la creazione di un dataset di immagini di facce, quindi è necessario identificare la faccia dell'identità interessata all'interno delle immagini ottenute. 
La successiva fase si occupa proprio di questo, ovvero identificare le facce nelle immagini associate alle varie identità. Tale compito viene svolta tramite un face detector, scritto in c++ grazie alla libreria dlib e con il quale è stato creato un eseguibile che per ciascuna immagine ritorna il coordinate delle bounding box relative alle facce trovati. Dlib utilizza un metodo di detection delle facce basato su HOG, histogram of gradient. Ovviamente, in alcune immagini non ci saranno facce, in altre ci sarà la facce dell'identità relative e in altre ancora ci saranno più facce, l'identità corretta associata alla query più altre persone presenti nella foto. Indipendentemente da questo, problema che sarà preso in considerazione successivamente, questa fase si occupa quindi di processare ciascuna immagine del dataset con un face detector ed ottenere le coordinate delle bounding box relative alle facce trovate e salvare tali coordinate su un database mantenendo ovviamente l'associazione con l'immagine e l'identità corrispondente.

- DUPLICATE REMOVAL
Chiunque nell'utilizzo quotidiano di un motore di ricerca di immagini, si sarà occorto che nei risultati sono spesso comprese immagini duplicate sia perchè fanno riferimento allo stesso indirizzo o perchè contengono la stessa immagine per salvata a due indirizzi differenti. Quindi dopo la creazione del dataset avrò un certo numero di immagini duplicate per ciascuna identità. Tale fenomeno non è desiderabile, poichè tali immagini dovranno essere utilizzate in una fase di training, che deve essere in grado di generalizzare il più possibile ciò che sta cercando di apprendere in modo poi da essere in grado di predire e classificare il più correttamente possibile. Immagini duplicate sono quindi informazioni duplicate che non aggiungo nulla al processo di generalizzazione, nel caso interessato, di una faccia di un'identità. Occorre quindi eliminare, o comunque tentare di ridurre il più possibile i duplicati nel dataset, attraverso una fase di duplicate removal. In questa face le immagini diventano un crop delle immagine originale relativo alla bounding box allargata, in modo prendere in considerazione oltre alla faccia anche un po' di contesto dello sfondo, ma non tutta l'immagine in modo da ridurre le esigenze computazionali, dato che il processo deve essere eseguito su potenzialmente milioni di immagini. Ciò viene fatto calcolando quindi per ciascuna immagine un descrittore usando VLAD. Tale descrittore viene utilizzato poi per clusterizzare le varie immagini in gruppi di immagini simili, in teoria quindi le immagini duplicate si ritroveranno nello stesso cluster. Per ciascun cluster viene selezionato solo un elemento, rimuovendo così i duplicati. Anche in questo caso tutte le informazione vengono salvate in una tabella del database, senza prendere nessuna decisione finale sulle immagini fisiche, ovvero non si cancellano immagini che sarebbero da scartare, perchè ci potrebbero sempre essere degli errori e visto gli sforzi richiesti per ottenere tali immagini e meglio andarci prudenti

- CLASSIFICAZIONE 
Arrivati a questo punto ho creat un dataset ed un database di informazioni, grazie alle quali posso ottenere un dataset di immagini contenenti facce, in cui i duplicati sono stati rimossi, associati a ciascuna identità. Ciò di cui non siamo ancora sicuri è proprio l'associazione fra facce e identità: l'immagine originale potrebbe contenere più facce dando quindi vita a più immagini contenenti ciascuna una faccia ma senza sapere quali di queste è da associare correttamente all'identità; inoltre il face detector potrebbe aver sbagliato ed avere identificato come faccia ciò che in realtà non lo è.
E' quindi necessaria una fase di classificazione delle immagini in modo da pulire il dataset da ciò che non è rilevante o sbagliato, ovvero che non contiene la faccia dell'identità relativa.
Innanzitutto, in questa fase le immagini sono le immagini estrapolate grazie alle coordinate della bounding box, in modo da concentrarsi sullo sul volto trovato. Sfruttando una cnn pre addestrata su 1000 identità con facce, viene estrapolato dalla rete l'ultimo layer di convoluzione come descrittore delle immagini. Successivamente vengono prese le 50 migliori immagini per ciascuna identità scelte in base al ranking di download, assumendo che le immagini più pertinenti e quindi contenti la facce dell'identità relativa abbiano ranking più alto. Tale 50 immagini per identità vengono utilizzate per addestrare un svm lineare. Per cercare di ridurre la complessità computazionale ciascuna identità viene addestrata contro altre k (5) identità scelte a caso. Ciò non compromette la precisione della classificazione per in questa fase non ci interessa classificare correttamente un'immagine associandola all'identità giusta, ma stabilire su un'immagine che si presuppone essere associata ad un'identità, lo è veramente. Quindi si tratta di una classificazione lineare per ciascuna identità e quindi dovremmo creare un modello, ovvero addestrare ciascuna identità separatamente. E' meno costo addestrare molti modelli ma con un dataset di dimensioni più contenute che creare un unico modello con un dataset di dimensioni enormi.
Quindi una volta terminato l'addestramento, otterrò un modello che utilizzerò per classificare le facce ottenute dalle fasi precendenti, per eliminare quelle che non appartengono alla relativa identità. I risultati della classificazione vengono salvati su un database.

- VALIDAZIONE
Ovviamente la fase di classificazione non è perfetta ma è soggetta ad un certo grado di precisione. Questo dipende da un problema intrinseco del machine learning, ma anche dalla possibilià che le immagini utilizzate per la fase di training non siano sufficientemente pulite. A due facce che stavano nella stessa immagini verrà assegnato lo stesso ranking, ma solo una faccia sarà effettivamente dell'identità corretta. Tuttavia, essendo impossibile distinguere nelle fasi precedenti questa differenza, se il ranking dell'immagine originale era alto, entrambe verranno utilizzate per il training, quindi avrò addestrato il modello con un dataset non pulito.
Per risolvere questi errori, e per valutare la precisione di questa procedura, è stato creato un metodo per la validazione visuale da parte di un utente umano dei risultati ottenuti dalle fasi precedenti e in particolare della classificazione. Per tale validazione visuale è stato creato una pagina web, grazie al quale vengono mostrate all'utente, per ciascuna identità le immagini associate ed i risultati della classicazione. Le immagini originali vengono croppate utilizzando le bounding box allargate, per permettere una corretta e più leggera visualizzazione sulla pagina web. Le immagini (facce) con un bordo rosso saranno quelle che il classificatore ha considerato non relative all'identità mentre quelle senza bordo le ha considerato relative all'identità relative. L'utente può interagire scorrendo le varie identità e/o le varie immagini mostrate in una galleria ma soprattutto tramite doppio click potrà correggere i risultati della classificazione. Facendo doppio click su un immagine senza bordo, questa diventerà con bordo rosso, ovvero l'utente afferma che tale immagine è stata scorrettamente classificata come relative all'identità in considerazione. Viceversa, facendo doppio click su un'immagine con bordo rosso, diventerà senza bordo, affermando quindi che tale immagine appartiene all'identità in considerazione, contrariamente a quanto predetto dalla fase di classificazione. Tale processo è più veloce di una annotazione standard perchè l'utente deve solo validare dei risultati, non stabilirli lui fin dall'inizio; inoltre è l'interfaccia è stata pensata per funzionare con un numero più limitato possibile di operazione da eseguire e per essere intuitiva. Anche i cambiamenti apportati alla classificazione da parte dell'utente in questa fase di validazione vengono salvati su un database. La classificazione e la validazione vengono poi confrontati per creare il groundtruth e quindi un dataset pulito di facce adatte per essere utilizzate nell'addestramento di un cnn.

- ESPERIMENTI
Per verificare la bontà di tale procedura sono stati fatti esperimenti con due diversi insiemi di dataset. Una contente identità quali celebrità, attori etc l'altro contente identità come calciatori. Le prime erano una selezione delle identità con cui la rete usa per estrarre il classificatore era stata addestrata.
Per verificare la precisione della classificazione sono state prese 50 identità del primo insieme e 50 dal secondo. Sono state eseguite tutte le fasi della procedura fino alla classificazione. Infine è stata effettuata la parte di validazione visuale tramite l'interfaccia web creata. Sfruttando così la validazione è stato possibile creare un groundtruth contro il quale confrontare i risultatai della procedura automatica fino alla classificazione. I risultati ottenuto sono promettenete (mostrare e rifare etc).

- CONCLUSIONE
La procedura implementa mostra risultati promettenti. Tuttavia ci sono diverse questioni da discutere ed aspetti da migliorare.
- instabilità nell'usare i motori di ricerca, parsin ad hoc, perchè possono essere soggetti a modifiche e quindi in futuro la pipeline così come è potrebbe non funzionare più, ma occorrerebbe riscrivere il parsing o cambiare motore di ricerca, infatti non tutti permetto di ottenere i link alle immagini originale, ma solo alle thumbnails, troppo piccole per essere usate.
- duplicate removal usando vlfeat migliorabile perchè ha solo kmeans o comunque clustering in cui bisogna prespecificare il numero di cluster da ottenere. Inoltre spesso le immagini vengono considerate diverse anche se contenutisticamente sono uguali, perchè ci sono differenze in termini di qualità, risoluzione, etc che rendono quindi i descrittori vlad suscettibli a tali differenze e quindi le immagini non vengono considerate uguale e messe nello stesso clusters, impendento una corretta rimozione del duplicato.
- problema già citato con la parte di training, ovvero dataset non pulito in generale.
- presenza di identità non ottimali, con nomi poco conosciuti o troppo generici producono insiemi di immagini troppo rumorose, rendono necessario eliminare tale identità dalla lista, cosa possibli nella fase di validazione facendo doppio click sull'immagine di riferimento dell'identità, che diventerà con sfondo rosso.
- procedura lenta, soprattutto dovuta al numero enorme di immagini che si vogliono ottenere, soprattutto nella fase di classificazione e validazione.

